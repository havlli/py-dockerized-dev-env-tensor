# Dockerized ML Development Environment (TensorFlow Base + PyTorch & Hugging Face)

This project provides a reproducible and GPU-accelerated development environment for Machine Learning tasks, orchestrated using Docker Compose. It leverages the official TensorFlow GPU image as a base and adds key libraries like PyTorch and Hugging Face Transformers, making it suitable for multi-framework development.

## Purpose

The main goal is to offer a consistent, containerized environment with essential ML tools pre-installed and configured, eliminating the need for manual setup on different machines. It's designed for developers working with TensorFlow, PyTorch, and the Hugging Face ecosystem, providing seamless access to GPU resources.

## Core Components & Features

*   üê≥ **Base Image:** Built upon the official `tensorflow/tensorflow:latest-gpu` image, ensuring TensorFlow and its CUDA dependencies are correctly configured.
*   <0xF0><0x9F><0xA7><0xB1> **Multi-Framework:** Includes PyTorch (`torch`, `torchvision`) and the Hugging Face suite (`transformers`, `datasets`, `accelerate`) alongside TensorFlow.
*   üìä **Essential Libraries:** Comes with `pandas`, `scikit-learn`, `matplotlib`, and `seaborn` for common data manipulation, ML tasks, and plotting.
*   üöÄ **GPU Acceleration:** Configured for NVIDIA GPU usage via Docker Compose and the base image.
*   üìì **Jupyter Notebook:** Provides an interactive development interface via JupyterLab/Notebook running within the main development container.
*   üí° **Optimized TensorBoard:** Runs TensorBoard in a separate, lightweight container (`python:3.11-slim`) for efficiency, accessing logs generated by the main development service.
*   ‚úÖ **Startup Checks:** Automatically runs environment verification scripts on startup to ensure core libraries (TensorFlow, PyTorch, Torchvision, Transformers) are installed and operational.
*   ‚öôÔ∏è **Docker Compose Orchestration:** Uses `docker-compose.yml` to define and manage the development (`dev`) and `tensorboard` services.

## Prerequisites

*   **Docker Engine:** [Install Docker](https://docs.docker.com/engine/install/)
*   **Docker Compose:** Typically included with Docker Desktop, otherwise [Install Compose](https://docs.docker.com/compose/install/).
*   **NVIDIA GPU:** A CUDA-compatible NVIDIA graphics card.
*   **NVIDIA Drivers:** Appropriate NVIDIA drivers installed on the host machine.
*   **GPU Passthrough Setup:**
    *   **Linux:** [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed.
    *   **Windows:** WSL 2 backend enabled in Docker Desktop with compatible NVIDIA drivers (supporting WSL 2) installed.

## Project Structure

```
.
‚îú‚îÄ‚îÄ Dockerfile            # Defines the main ML development image (FROM tensorflow:latest-gpu)
‚îú‚îÄ‚îÄ Dockerfile.tensorboard # Defines the lightweight image for the TensorBoard service
‚îú‚îÄ‚îÄ docker-compose.yml    # Orchestrates the 'dev' and 'tensorboard' services
‚îú‚îÄ‚îÄ requirements.txt      # Lists *additional* Python packages installed on top of the base TF image
‚îú‚îÄ‚îÄ env-test/             # Contains scripts to verify the environment setup
‚îÇ   ‚îú‚îÄ‚îÄ run_all_tests.py  # Script to execute all checks (run on startup)
‚îÇ   ‚îú‚îÄ‚îÄ tensor_test.py    # TensorFlow check script
‚îÇ   ‚îú‚îÄ‚îÄ torch_test.py     # PyTorch check script
‚îÇ   ‚îú‚îÄ‚îÄ tvision_test.py   # Torchvision check script
‚îÇ   ‚îî‚îÄ‚îÄ transformers_test.py # Hugging Face Transformers check script
‚îú‚îÄ‚îÄ src/                    # Example directory for your project source code
‚îú‚îÄ‚îÄ notebooks/             # Example directory for your Jupyter notebooks
‚îú‚îÄ‚îÄ data/                  # Example directory for datasets
‚îî‚îÄ‚îÄ logs/                  # Directory for storing training logs (used by TensorBoard)
```

## Getting Started

1.  **Clone the Repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```
2.  **Build and Start Services:**
    Run the following command from the project root directory:
    ```bash
    docker-compose up --build -d
    ```
    *   `--build`: Ensures the images are built (or rebuilt if changes were made).
    *   `-d`: Runs the containers in detached mode (in the background).

3.  **Startup Checks:** When the `dev-environment` container starts, it will automatically execute the scripts in `env-test/` via `run_all_tests.py`. Check the logs to ensure all tests pass:
    ```bash
    docker-compose logs -f dev-environment
    ```
    Look for `[OVERALL SUCCESS] All environment checks passed.` before the Jupyter startup messages.

4.  **Access Services:**
    *   **Jupyter Notebook:** Open your web browser to `http://localhost:8888`
    *   **TensorBoard:** Open your web browser to `http://localhost:6006`

## Development Workflow

*   **Primary Container:** Most development happens within the `dev-environment` container, accessed via Jupyter or `docker-compose exec`.
*   **Executing Commands:** To run commands inside the main development container:
    ```bash
    docker-compose exec dev-environment <your_command>
    # Example: Run the environment checks manually
    docker-compose exec dev-environment python /app/env-test/run_all_tests.py
    # Example: Get an interactive shell
    docker-compose exec dev-environment bash
    ```
*   **Adding Python Dependencies:**
    1.  Add the required package(s) to `requirements.txt`.
    2.  Rebuild the `dev` service image: `docker-compose build dev-environment` or simply run `docker-compose up --build -d` again.
*   **Adding System Dependencies:** If you need tools installable via `apt-get`:
    1.  Modify the `Dockerfile` to include the necessary `apt-get install` commands (remember `apt-get update` first and potentially cleanup afterwards).
    2.  Rebuild the image: `docker-compose build dev-environment` or `docker-compose up --build -d`.
*   **Stopping Services:**
    ```bash
    docker-compose down
    ```
    Use `docker-compose down -v` to also remove the volumes (if any were defined as named volumes, which is not the case here).

## GPU Support

GPU access is enabled through:
1.  Using the `tensorflow/tensorflow:latest-gpu` base image which includes CUDA libraries.
2.  The `deploy` section within the `dev` service definition in `docker-compose.yml`, which requests GPU resources from Docker.
3.  Proper configuration of NVIDIA drivers and the NVIDIA Container Toolkit / WSL 2 setup on the host machine.

The environment test scripts (`tensor_test.py`, `torch_test.py`) include checks for GPU availability and perform simple operations to verify access.

## TensorBoard Service

The `tensorboard` service runs in a separate, minimal container defined by `Dockerfile.tensorboard`. It mounts the same project directory (`.:/app`) as the `dev` service, allowing it to read log files written to the `/app/logs` directory (corresponding to the `logs/` folder on your host) by your training scripts running in the `dev` container.

## License

This project template is available under the MIT License. 